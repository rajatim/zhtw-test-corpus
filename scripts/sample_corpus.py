#!/usr/bin/env python3
"""
å¾å¤§å‹èªæ–™åº«æŠ½æ¨£ç”Ÿæˆæ¸¬è©¦è³‡æ–™

ä½¿ç”¨æ–¹å¼ï¼š
    python scripts/sample_corpus.py --source large/ --output samples/
    python scripts/sample_corpus.py --source large/ --count 100

åŠŸèƒ½ï¼š
    1. å¾ä¸‹è¼‰çš„èªæ–™åº«éš¨æ©ŸæŠ½æ¨£
    2. è½‰æ›ç‚º zhtw-test-corpus JSON æ ¼å¼
    3. å¯é¸æ“‡ä½¿ç”¨ zhtw ç”Ÿæˆé æœŸè¼¸å‡ºï¼ˆéœ€äººå·¥æ ¡é©—ï¼‰
"""

import argparse
import json
import random
import re
from datetime import datetime
from pathlib import Path


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    text = re.sub(r'\s+', ' ', text.strip())
    # ç§»é™¤ HTML æ¨™ç±¤
    text = re.sub(r'<[^>]+>', '', text)
    # é™åˆ¶é•·åº¦
    if len(text) > 500:
        # æ‰¾åˆ°å¥è™Ÿä½ç½®æˆªæ–·
        pos = text.find('ã€‚', 100)
        if pos > 0:
            text = text[:pos + 1]
        else:
            text = text[:200] + '...'
    return text


def has_simplified_chinese(text: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦åŒ…å«ç°¡é«”ä¸­æ–‡ï¼ˆç°¡å–®åˆ¤æ–·ï¼‰"""
    # å¸¸è¦‹ç°¡é«”å­—
    simplified_chars = set('ç®€ä½“å›½é™…å‘è¿™ä¸ºä¸ªç€æ—¶ä¼šç§é•¿æ¥ä¸œè¯´å¯¹åŠ¨æœºå…³è¿›ç»ç»™å­¦å®ç°ç‚¹å¼€é—®é¢˜è¿˜æ ·')
    return any(c in simplified_chars for c in text)


def sample_wiki(source_dir: Path, count: int) -> list:
    """å¾ç¶­åŸºç™¾ç§‘æŠ½æ¨£"""
    samples = []
    # å˜—è©¦ä¸åŒå¯èƒ½çš„ç›®éŒ„åç¨±
    possible_dirs = ["wiki_zh", "wiki2019zh", "wiki"]
    wiki_dir = None
    for name in possible_dirs:
        if (source_dir / name).exists():
            wiki_dir = source_dir / name
            break

    if wiki_dir is None:
        print(f"âš ï¸ æ‰¾ä¸åˆ° wiki èªæ–™ï¼Œè·³é")
        return samples

    # æ‰¾æ‰€æœ‰æª”æ¡ˆï¼ˆå¯èƒ½æ˜¯ JSON æˆ– wiki_00 æ ¼å¼ï¼‰
    json_files = list(wiki_dir.glob("*.json")) + list(wiki_dir.glob("**/*.json"))
    if not json_files:
        # å˜—è©¦æ‰¾ wiki_xx æ ¼å¼æª”æ¡ˆï¼ˆæ¯è¡Œæ˜¯ JSONï¼‰
        json_files = list(wiki_dir.glob("**/wiki_*"))
    if not json_files:
        # å˜—è©¦æ‰¾ txt æª”
        json_files = list(wiki_dir.glob("*.txt"))

    print(f"   æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ")

    all_texts = []
    for f in json_files[:10]:  # åªè®€å‰ 10 å€‹æª”æ¡ˆ
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                for line in fp:
                    try:
                        data = json.loads(line.strip())
                        text = data.get('text', '') or data.get('content', '')
                        if text and len(text) > 50 and has_simplified_chinese(text):
                            all_texts.append(clean_text(text))
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"   è®€å– {f.name} å¤±æ•—: {e}")

    # éš¨æ©ŸæŠ½æ¨£
    if all_texts:
        sampled = random.sample(all_texts, min(count, len(all_texts)))
        for i, text in enumerate(sampled):
            samples.append({
                "id": f"wiki_{i+1:03d}",
                "input": text,
                "expected": "",  # éœ€äººå·¥å¡«å¯«
                "tags": ["wiki", "encyclopedia"],
                "notes": "è‡ªå‹•æŠ½æ¨£ï¼Œéœ€äººå·¥æ ¡é©— expected",
            })

    return samples


def sample_news(source_dir: Path, count: int) -> list:
    """å¾æ–°èèªæ–™æŠ½æ¨£"""
    samples = []
    news_dir = source_dir / "news2016zh"

    if not news_dir.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ° news2016zhï¼Œè·³é")
        return samples

    json_files = list(news_dir.glob("*.json")) + list(news_dir.glob("**/*.json"))
    print(f"   æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ")

    all_texts = []
    for f in json_files[:10]:
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                for line in fp:
                    try:
                        data = json.loads(line.strip())
                        # æ–°èæ ¼å¼ï¼štitle, content, desc
                        title = data.get('title', '')
                        content = data.get('content', '') or data.get('desc', '')
                        text = f"{title}ã€‚{content}" if title else content
                        if text and len(text) > 50 and has_simplified_chinese(text):
                            all_texts.append(clean_text(text))
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"   è®€å– {f.name} å¤±æ•—: {e}")

    if all_texts:
        sampled = random.sample(all_texts, min(count, len(all_texts)))
        for i, text in enumerate(sampled):
            samples.append({
                "id": f"news_{i+1:03d}",
                "input": text,
                "expected": "",
                "tags": ["news", "formal"],
                "notes": "è‡ªå‹•æŠ½æ¨£ï¼Œéœ€äººå·¥æ ¡é©— expected",
            })

    return samples


def sample_webtext(source_dir: Path, count: int) -> list:
    """å¾ç¤¾å€å•ç­”æŠ½æ¨£"""
    samples = []
    webtext_dir = source_dir / "webtext2019zh"

    if not webtext_dir.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ° webtext2019zhï¼Œè·³é")
        return samples

    json_files = list(webtext_dir.glob("*.json")) + list(webtext_dir.glob("**/*.json"))
    print(f"   æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ")

    all_texts = []
    for f in json_files[:10]:
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                for line in fp:
                    try:
                        data = json.loads(line.strip())
                        # å•ç­”æ ¼å¼
                        question = data.get('title', '') or data.get('question', '')
                        answer = data.get('content', '') or data.get('answer', '')
                        if question and has_simplified_chinese(question):
                            all_texts.append(clean_text(question))
                        if answer and has_simplified_chinese(answer):
                            all_texts.append(clean_text(answer))
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"   è®€å– {f.name} å¤±æ•—: {e}")

    if all_texts:
        sampled = random.sample(all_texts, min(count, len(all_texts)))
        for i, text in enumerate(sampled):
            samples.append({
                "id": f"social_{i+1:03d}",
                "input": text,
                "expected": "",
                "tags": ["social", "qa", "informal"],
                "notes": "è‡ªå‹•æŠ½æ¨£ï¼Œéœ€äººå·¥æ ¡é©— expected",
            })

    return samples


def sample_baike(source_dir: Path, count: int) -> list:
    """å¾ç™¾ç§‘å•ç­”æŠ½æ¨£"""
    samples = []
    baike_dir = source_dir / "baike2018qa"

    if not baike_dir.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ° baike2018qaï¼Œè·³é")
        return samples

    json_files = list(baike_dir.glob("*.json")) + list(baike_dir.glob("**/*.json"))
    print(f"   æ‰¾åˆ° {len(json_files)} å€‹æª”æ¡ˆ")

    all_texts = []
    for f in json_files[:10]:
        try:
            with open(f, 'r', encoding='utf-8') as fp:
                for line in fp:
                    try:
                        data = json.loads(line.strip())
                        question = data.get('title', '') or data.get('question', '')
                        answer = data.get('answer', '') or data.get('content', '')
                        text = f"å•ï¼š{question} ç­”ï¼š{answer}" if question and answer else (question or answer)
                        if text and len(text) > 30 and has_simplified_chinese(text):
                            all_texts.append(clean_text(text))
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            print(f"   è®€å– {f.name} å¤±æ•—: {e}")

    if all_texts:
        sampled = random.sample(all_texts, min(count, len(all_texts)))
        for i, text in enumerate(sampled):
            samples.append({
                "id": f"baike_{i+1:03d}",
                "input": text,
                "expected": "",
                "tags": ["baike", "qa", "encyclopedia"],
                "notes": "è‡ªå‹•æŠ½æ¨£ï¼Œéœ€äººå·¥æ ¡é©— expected",
            })

    return samples


def generate_expected_with_zhtw(samples: list) -> list:
    """ä½¿ç”¨ zhtw ç”Ÿæˆé æœŸè¼¸å‡ºï¼ˆéœ€å·²å®‰è£ zhtwï¼‰"""
    try:
        from zhtw.dictionary import load_dictionary
        from zhtw.matcher import Matcher

        terms = load_dictionary(sources=["cn", "hk"])
        matcher = Matcher(terms)

        for sample in samples:
            if not sample["expected"]:
                sample["expected"] = matcher.replace_all(sample["input"])
                sample["notes"] = "ç”± zhtw è‡ªå‹•ç”Ÿæˆï¼Œéœ€äººå·¥æ ¡é©—"

        print("âœ… å·²ä½¿ç”¨ zhtw ç”Ÿæˆé æœŸè¼¸å‡º")
    except ImportError:
        print("âš ï¸ æœªå®‰è£ zhtwï¼Œexpected æ¬„ä½ä¿æŒç©ºç™½")
        print("   å®‰è£æ–¹å¼: pip install zhtw")

    return samples


def save_samples(samples: list, output_path: Path, category: str):
    """å„²å­˜æ¨£æœ¬ç‚º JSON"""
    output_data = {
        "metadata": {
            "source": "brightmart/nlp_chinese_corpus",
            "source_url": "https://github.com/brightmart/nlp_chinese_corpus",
            "license": "å¾…ç¢ºèª",
            "collected_date": datetime.now().strftime("%Y-%m-%d"),
            "description": f"{category} èªæ–™è‡ªå‹•æŠ½æ¨£",
            "auto_generated": True,
        },
        "corpus": samples,
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    print(f"   ğŸ’¾ å„²å­˜ {len(samples)} æ¢åˆ° {output_path}")


def main():
    parser = argparse.ArgumentParser(description="å¾å¤§å‹èªæ–™åº«æŠ½æ¨£ç”Ÿæˆæ¸¬è©¦è³‡æ–™")
    parser.add_argument(
        "--source",
        type=Path,
        default=Path(__file__).parent.parent / "large",
        help="èªæ–™åº«ä¾†æºç›®éŒ„ï¼ˆé è¨­: large/ï¼‰",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path(__file__).parent.parent / "samples",
        help="è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: samples/ï¼‰",
    )
    parser.add_argument(
        "--count",
        type=int,
        default=50,
        help="æ¯å€‹é¡åˆ¥æŠ½æ¨£æ•¸é‡ï¼ˆé è¨­: 50ï¼‰",
    )
    parser.add_argument(
        "--use-zhtw",
        action="store_true",
        help="ä½¿ç”¨ zhtw è‡ªå‹•ç”Ÿæˆ expectedï¼ˆä»éœ€äººå·¥æ ¡é©—ï¼‰",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éš¨æ©Ÿç¨®å­ï¼ˆé è¨­: 42ï¼‰",
    )

    args = parser.parse_args()
    random.seed(args.seed)

    print(f"ğŸ“ èªæ–™ä¾†æº: {args.source}")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {args.output}")
    print(f"ğŸ“Š æ¯é¡åˆ¥æŠ½æ¨£: {args.count} æ¢")
    print()

    if not args.source.exists():
        print(f"âŒ ä¾†æºç›®éŒ„ä¸å­˜åœ¨: {args.source}")
        print(f"   è«‹å…ˆåŸ·è¡Œ: python scripts/download_corpus.py --recommended")
        return

    # å„é¡åˆ¥æŠ½æ¨£
    all_samples = {}

    print("ğŸ“– æŠ½æ¨£ç¶­åŸºç™¾ç§‘...")
    all_samples["wiki"] = sample_wiki(args.source, args.count)

    print("ğŸ“° æŠ½æ¨£æ–°èèªæ–™...")
    all_samples["news"] = sample_news(args.source, args.count)

    print("ğŸ’¬ æŠ½æ¨£ç¤¾å€å•ç­”...")
    all_samples["social"] = sample_webtext(args.source, args.count)

    print("ğŸ“š æŠ½æ¨£ç™¾ç§‘å•ç­”...")
    all_samples["baike"] = sample_baike(args.source, args.count)

    # ä½¿ç”¨ zhtw ç”Ÿæˆ expected
    if args.use_zhtw:
        print("\nğŸ”„ ä½¿ç”¨ zhtw ç”Ÿæˆé æœŸè¼¸å‡º...")
        for category, samples in all_samples.items():
            all_samples[category] = generate_expected_with_zhtw(samples)

    # å„²å­˜
    print("\nğŸ’¾ å„²å­˜æ¨£æœ¬...")
    total = 0
    for category, samples in all_samples.items():
        if samples:
            save_samples(samples, args.output / category / "sampled.json", category)
            total += len(samples)

    print(f"\nâœ… å®Œæˆï¼å…±æŠ½æ¨£ {total} æ¢")
    print(f"\nâš ï¸ é‡è¦ï¼šexpected æ¬„ä½éœ€è¦äººå·¥æ ¡é©—ï¼")
    print(f"   å³ä½¿ä½¿ç”¨ --use-zhtwï¼Œä»å¯èƒ½æœ‰èª¤è½‰ï¼Œè«‹äººå·¥ç¢ºèªã€‚")


if __name__ == "__main__":
    main()
