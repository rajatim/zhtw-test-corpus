#!/usr/bin/env python3
"""
ä¸‹è¼‰ä¸­æ–‡èªæ–™åº«

è³‡æ–™ä¾†æºï¼šhttps://github.com/brightmart/nlp_chinese_corpus

ä½¿ç”¨æ–¹å¼ï¼š
    pip install gdown
    python scripts/download_corpus.py [--dataset DATASET] [--all]

ç¯„ä¾‹ï¼š
    python scripts/download_corpus.py --dataset wiki      # åªä¸‹è¼‰ç¶­åŸºç™¾ç§‘
    python scripts/download_corpus.py --dataset news      # åªä¸‹è¼‰æ–°èèªæ–™
    python scripts/download_corpus.py --all               # ä¸‹è¼‰å…¨éƒ¨ï¼ˆéœ€å¤§é‡ç©ºé–“ï¼‰
"""

import argparse
import os
import subprocess
import sys
from pathlib import Path

# Google Drive æª”æ¡ˆ ID
DATASETS = {
    "wiki": {
        "name": "ç¶­åŸºç™¾ç§‘ (wiki2019zh)",
        "gdrive_id": "1EdHUZIDpgcBoSqbjlfNKJ3b1t0XIUjbt",
        "filename": "wiki2019zh.zip",
        "size": "519MB",
        "description": "104è¬æ¢ç¶­åŸºè©æ¢",
    },
    "news": {
        "name": "æ–°èèªæ–™ (news2016zh)",
        "gdrive_id": "1TMKu1FpTr6kcjWXWlQHX7YJsMfhhcVKp",
        "filename": "news2016zh.zip",
        "size": "3.6GB",
        "description": "250è¬ç¯‡æ–°èæ–‡ç« ",
    },
    "baike": {
        "name": "ç™¾ç§‘å•ç­” (baike2018qa)",
        "gdrive_id": "1_vgGQZpfSxN_Ng9iTAvE7hM3Z7NVwXP2",
        "filename": "baike2018qa.zip",
        "size": "663MB",
        "description": "150è¬å•ç­”å°",
    },
    "webtext": {
        "name": "ç¤¾å€å•ç­” (webtext2019zh)",
        "gdrive_id": "1u2yW_XohbYL2YAK6Bzc5XrngHstQTf0v",
        "filename": "webtext2019zh.zip",
        "size": "1.7GB",
        "description": "410è¬ç¤¾å€å•ç­”",
    },
    "translation": {
        "name": "ç¿»è­¯èªæ–™ (translation2019zh)",
        "gdrive_id": "1EX8eE5YWBxCaohBO8Fh4e2j3b9C2bTVQ",
        "filename": "translation2019zh.zip",
        "size": "596MB",
        "description": "520è¬ä¸­è‹±ç¿»è­¯å°",
    },
}

# æ¨è–¦çš„æœ€å°æ¸¬è©¦é›†
RECOMMENDED = ["wiki", "news"]


def check_gdown():
    """æª¢æŸ¥ gdown æ˜¯å¦å·²å®‰è£"""
    try:
        import gdown  # noqa: F401
        return True
    except ImportError:
        return False


def install_gdown():
    """å®‰è£ gdown"""
    print("æ­£åœ¨å®‰è£ gdown...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])


def download_dataset(dataset_key: str, output_dir: Path, force: bool = False):
    """ä¸‹è¼‰å–®ä¸€è³‡æ–™é›†"""
    import gdown

    if dataset_key not in DATASETS:
        print(f"âŒ æœªçŸ¥çš„è³‡æ–™é›†: {dataset_key}")
        print(f"   å¯ç”¨é¸é …: {', '.join(DATASETS.keys())}")
        return False

    dataset = DATASETS[dataset_key]
    output_path = output_dir / dataset["filename"]
    extracted_dir = output_dir / dataset_key

    # æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰
    if extracted_dir.exists() and not force:
        print(f"âœ… {dataset['name']} å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
        return True

    print(f"\nğŸ“¥ ä¸‹è¼‰ {dataset['name']}")
    print(f"   å¤§å°: {dataset['size']}")
    print(f"   èªªæ˜: {dataset['description']}")

    # ä¸‹è¼‰
    url = f"https://drive.google.com/uc?id={dataset['gdrive_id']}"
    try:
        gdown.download(url, str(output_path), quiet=False)
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        print(f"   è«‹æ‰‹å‹•ä¸‹è¼‰: https://drive.google.com/file/d/{dataset['gdrive_id']}/view")
        return False

    # è§£å£“ç¸®
    if output_path.exists():
        print(f"ğŸ“¦ è§£å£“ç¸® {output_path.name}...")
        import zipfile
        try:
            with zipfile.ZipFile(output_path, 'r') as zip_ref:
                zip_ref.extractall(output_dir)
            # åˆªé™¤ zip æª”
            output_path.unlink()
            print(f"âœ… {dataset['name']} ä¸‹è¼‰å®Œæˆ")
            return True
        except zipfile.BadZipFile:
            print(f"âŒ è§£å£“ç¸®å¤±æ•—ï¼Œæª”æ¡ˆå¯èƒ½æå£")
            return False

    return False


def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è³‡æ–™é›†"""
    print("\nå¯ç”¨çš„è³‡æ–™é›†ï¼š\n")
    print(f"{'åç¨±':<12} {'å¤§å°':<8} {'èªªæ˜'}")
    print("-" * 60)
    for key, ds in DATASETS.items():
        rec = " â­" if key in RECOMMENDED else ""
        print(f"{key:<12} {ds['size']:<8} {ds['description']}{rec}")
    print("\nâ­ = å»ºè­°ä¸‹è¼‰ï¼ˆæ¶µè“‹æœ€å¸¸è¦‹å ´æ™¯ï¼‰")
    print(f"\nç¸½è¨ˆ: {len(DATASETS)} å€‹è³‡æ–™é›†")


def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è¼‰ä¸­æ–‡ NLP èªæ–™åº«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
    %(prog)s --list                    # åˆ—å‡ºæ‰€æœ‰è³‡æ–™é›†
    %(prog)s --dataset wiki            # ä¸‹è¼‰ç¶­åŸºç™¾ç§‘
    %(prog)s --dataset wiki news       # ä¸‹è¼‰ç¶­åŸºå’Œæ–°è
    %(prog)s --recommended             # ä¸‹è¼‰å»ºè­°è³‡æ–™é›†
    %(prog)s --all                     # ä¸‹è¼‰å…¨éƒ¨ï¼ˆç´„ 7GBï¼‰
        """,
    )
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„è³‡æ–™é›†")
    parser.add_argument("--dataset", nargs="+", help="æŒ‡å®šè¦ä¸‹è¼‰çš„è³‡æ–™é›†")
    parser.add_argument("--recommended", action="store_true", help="ä¸‹è¼‰å»ºè­°çš„è³‡æ–™é›†")
    parser.add_argument("--all", action="store_true", help="ä¸‹è¼‰æ‰€æœ‰è³‡æ–™é›†")
    parser.add_argument("--force", action="store_true", help="å¼·åˆ¶é‡æ–°ä¸‹è¼‰")
    parser.add_argument(
        "--output",
        type=Path,
        default=Path(__file__).parent.parent / "large",
        help="è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: large/ï¼‰",
    )

    args = parser.parse_args()

    if args.list:
        list_datasets()
        return

    # ç¢ºä¿ gdown å·²å®‰è£
    if not check_gdown():
        install_gdown()

    # æ±ºå®šè¦ä¸‹è¼‰çš„è³‡æ–™é›†
    datasets_to_download = []
    if args.all:
        datasets_to_download = list(DATASETS.keys())
    elif args.recommended:
        datasets_to_download = RECOMMENDED
    elif args.dataset:
        datasets_to_download = args.dataset
    else:
        parser.print_help()
        print("\nè«‹æŒ‡å®šè¦ä¸‹è¼‰çš„è³‡æ–™é›†ï¼Œæˆ–ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨é¸é …")
        return

    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    args.output.mkdir(parents=True, exist_ok=True)

    print(f"\nğŸ“ è¼¸å‡ºç›®éŒ„: {args.output}")
    print(f"ğŸ“Š å°‡ä¸‹è¼‰ {len(datasets_to_download)} å€‹è³‡æ–™é›†: {', '.join(datasets_to_download)}")

    # ä¸‹è¼‰
    success = 0
    for ds in datasets_to_download:
        if download_dataset(ds, args.output, args.force):
            success += 1

    print(f"\nâœ… å®Œæˆ: {success}/{len(datasets_to_download)} å€‹è³‡æ–™é›†ä¸‹è¼‰æˆåŠŸ")

    if success > 0:
        print(f"\nä¸‹ä¸€æ­¥ï¼šåŸ·è¡ŒæŠ½æ¨£è…³æœ¬ç”Ÿæˆæ¸¬è©¦è³‡æ–™")
        print(f"    python scripts/sample_corpus.py --source {args.output}")


if __name__ == "__main__":
    main()
